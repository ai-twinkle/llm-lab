<div align="left" style="line-height: 1;">
  <a href="https://discord.gg/Cx737yw4ed" target="_blank" style="margin: 2px;">
    <img alt="Discord" src="https://img.shields.io/badge/Discord-Twinkle%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>
  </a>
  <a href="https://huggingface.co/twinkle-ai" target="_blank" style="margin: 2px;">
    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Twinkle%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>
  </a>
</div>

# 用 Twinkle Eval 評測大型語言模型 (LLM)

隨著大型語言模型（LLM）的快速演進，如何在短時間內有效且公平地評估它們，成為工程與產品決策者的重要課題。為此，**Twinkle AI 社群**開發了 **Twinkle Eval** — 一款專為大規模推理模型設計的開源自動化評測工具。

## 🌟 課程簡介
Twinkle Eval 結合 **平行批次處理** 和 **多輪隨機化測試** 方法，顯著提升評測效率，並降低模型因選項順序所產生的偏差。它支援多種格式輸入（如 CSV、JSON、Parquet）、可自訂生成參數與請求策略，並安全地處理 API 金鑰等敏感資料。

此外，該工具已內建 **台灣通識與法律題庫**（如 TMMLU+、tw-legal-benchmark-v1）與 **通用 MMLU**，適用於多場域測評需求。

## 🎯 你將學到
- 為什麼評測對 LLM 決策與研發至關重要  
- 如何利用 **平行化與隨機化** 大幅提升評測效率與公平性  
- 使用 Twinkle Eval 操作 **多格式輸入** 和 **參數設定**  
- 掌握 **統計分析與結果詮釋** 的基礎  

## 🚀 課程亮點
- 實作演練：從單題推論到平行批次，親手跑完一個完整的評測流程  
- 深入理解：如何避免選項順序偏差，並透過多輪測試獲得可信結果  
- 符合產業需求：整合台灣特有題庫與國際通用基準  

## 參與方式
課程將提供完整的 **Jupyter/Colab Notebooks**，學員可直接上手實作。  

---

> **Twinkle AI 社群**致力於推廣開源 AI 技術，本課程所有範例與工具將公開於 [GitHub](https://github.com/ai-twinkle)。